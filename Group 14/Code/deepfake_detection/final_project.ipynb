{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "00FTZhlvGYdj"
      },
      "source": [
        "# **Frame_Extractor**\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class VideoReader:\n",
        "\n",
        "    def __init__(self, verbose=True, insets=(0, 0)):\n",
        "        \n",
        "        self.verbose = verbose\n",
        "        self.insets = insets\n",
        "\n",
        "    def read_frames(self, path, num_frames, jitter=0, seed=None):\n",
        "      # Reads frames that are always evenly spaced throughout the video.\n",
        "      #     jitter: if not 0, adds small random offsets to the frame indices;\n",
        "      #         this is useful so we don't always land on even or odd frames\n",
        "      #     seed: random seed for jittering; if you set this to a fixed value,\n",
        "      #         you probably want to set it only on the first video \n",
        "       \n",
        "        assert num_frames > 0\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=np.int)\n",
        "        if jitter > 0:\n",
        "            np.random.seed(seed)\n",
        "            jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
        "            frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
        "\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_random_frames(self, path, num_frames, seed=None):\n",
        "       #Picks the frames at random.\n",
        "       \n",
        "        assert num_frames > 0\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = sorted(np.random.choice(np.arange(0, frame_count), num_frames))\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frames_at_indices(self, path, frame_idxs):\n",
        "      #Reads frames from a video and puts them into a NumPy array\n",
        "      #Returns:\n",
        "           # - a NumPy array of shape (num_frames, height, width, 3)\n",
        "           # - a list of the frame indices that were read\n",
        "        assert len(frame_idxs) > 0\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frames_at_indices(self, path, capture, frame_idxs):\n",
        "      #capture frames at indices for processing\n",
        "      \n",
        "        try:\n",
        "            frames = []\n",
        "            idxs_read = []\n",
        "            for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
        "                # Get the next frame, but don't decode if we're not using it.\n",
        "                ret = capture.grab()\n",
        "                if not ret:\n",
        "                    if self.verbose:\n",
        "                        print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n",
        "                    break\n",
        "\n",
        "                # Need to look at this frame?\n",
        "                current = len(idxs_read)\n",
        "                if frame_idx == frame_idxs[current]:\n",
        "                    ret, frame = capture.retrieve()\n",
        "                    if not ret or frame is None:\n",
        "                        if self.verbose:\n",
        "                            print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "                        break\n",
        "\n",
        "                    frame = self._postprocess_frame(frame)\n",
        "                    frames.append(frame)\n",
        "                    idxs_read.append(frame_idx)\n",
        "\n",
        "            if len(frames) > 0:\n",
        "                return np.stack(frames), idxs_read\n",
        "            if self.verbose:\n",
        "                print(\"No frames read from movie %s\" % path)\n",
        "            return None\n",
        "        except:\n",
        "            if self.verbose:\n",
        "                print(\"Exception while reading movie %s\" % path)\n",
        "            return None    \n",
        "\n",
        "    def read_middle_frame(self, path):\n",
        "      #Reads the frame from the middle of the video\n",
        "        \n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        result = self._read_frame_at_index(path, capture, frame_count // 2)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frame_at_index(self, path, frame_idx):\n",
        "      #this is the more efficient way to read single frame from a video\n",
        "        \n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frame_at_index(path, capture, frame_idx)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frame_at_index(self, path, capture, frame_idx):\n",
        "        capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = capture.read()    \n",
        "        if not ret or frame is None:\n",
        "            if self.verbose:\n",
        "                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "            return None\n",
        "        else:\n",
        "            frame = self._postprocess_frame(frame)\n",
        "            return np.expand_dims(frame, axis=0), [frame_idx]\n",
        "    \n",
        "    def _postprocess_frame(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.insets[0] > 0:\n",
        "            W = frame.shape[1]\n",
        "            p = int(W * self.insets[0])\n",
        "            frame = frame[:, p:-p, :]\n",
        "\n",
        "        if self.insets[1] > 0:\n",
        "            H = frame.shape[1]\n",
        "            q = int(H * self.insets[1])\n",
        "            frame = frame[q:-q, :, :]\n",
        "\n",
        "        return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFfZ3z5eGZ0E"
      },
      "source": [
        "# **FaceExtractor**\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class FaceExtractor:\n",
        "    \n",
        "    def __init__(self, video_read_fn, facedet):\n",
        "      # video_read_fn: a function that takes in a path to a video file\n",
        "      #           and returns a tuple consisting of a NumPy array with shape\n",
        "      #           (num_frames, H, W, 3) and a list of frame indices, or None\n",
        "      #           in case of an error\n",
        "      #       facedet: the face detector object\n",
        "       \n",
        "        self.video_read_fn = video_read_fn\n",
        "        self.facedet = facedet\n",
        "    \n",
        "    def process_videos(self, input_dir, filenames, video_idxs):\n",
        "      # For the specified selection of videos, grabs one or more frames \n",
        "      #   from each video, runs the face detector, and tries to find the faces \n",
        "      #   in each frame.\n",
        "        \n",
        "        target_size = self.facedet.input_size\n",
        "        videos_read = []\n",
        "        frames_read = []\n",
        "        frames = []\n",
        "        tiles = []\n",
        "        resize_info = []\n",
        "\n",
        "        for video_idx in video_idxs:\n",
        "            # Read the full-size frames from this video.\n",
        "            filename = filenames[video_idx]\n",
        "            video_path = os.path.join(input_dir, filename)\n",
        "            result = self.video_read_fn(video_path)\n",
        "\n",
        "            \n",
        "            if result is None: continue\n",
        "\n",
        "            videos_read.append(video_idx)\n",
        "\n",
        "           # Keep track of the original frames (need them later).\n",
        "            my_frames, my_idxs = result\n",
        "            frames.append(my_frames)\n",
        "            frames_read.append(my_idxs)\n",
        "           # Split the frames into several tiles. Resize the tiles to 128x128\n",
        "            my_tiles, my_resize_info = self._tile_frames(my_frames, target_size)\n",
        "            tiles.append(my_tiles)\n",
        "            resize_info.append(my_resize_info)\n",
        "            # Put all the tiles for all the frames from all the videos into\n",
        "             # a single batch.\n",
        "\n",
        "        batch = np.concatenate(tiles)\n",
        "        # Run the face detector. The result is a list of PyTorch tensors, \n",
        "        # one for each image in the batch.\n",
        "        all_detections = self.facedet.predict_on_batch(batch, apply_nms=False)\n",
        "\n",
        "        result = []\n",
        "        offs = 0\n",
        "        for v in range(len(tiles)):\n",
        "            num_tiles = tiles[v].shape[0]\n",
        "            detections = all_detections[offs:offs + num_tiles]\n",
        "            offs += num_tiles\n",
        "            detections = self._resize_detections(detections, target_size, resize_info[v])\n",
        "            # Because we have several tiles for each frame, combine the predictions\n",
        "            # from these tiles. The result is a list of PyTorch tensors,\n",
        "            num_frames = frames[v].shape[0]\n",
        "            frame_size = (frames[v].shape[2], frames[v].shape[1])\n",
        "            detections = self._untile_detections(num_frames, frame_size, detections)\n",
        "            # The same face may have been detected in multiple tiles, so filter out\n",
        "            # overlapping detections. This is done separately for each frame.\n",
        "            detections = self.facedet.nms(detections)\n",
        "\n",
        "            for i in range(len(detections)):\n",
        "               # Crop the faces out of  frame\n",
        "                faces = self._add_margin_to_detections(detections[i], frame_size, 0.2)\n",
        "                faces = self._crop_faces(frames[v][i], faces)\n",
        "                # Add additional information about the frame and detections\n",
        "                scores = list(detections[i][:, 16].cpu().numpy())\n",
        "                frame_dict = { \"video_idx\": videos_read[v],\n",
        "                               \"frame_idx\": frames_read[v][i],\n",
        "                               \"frame_w\": frame_size[0],\n",
        "                               \"frame_h\": frame_size[1],\n",
        "                               \"faces\": faces, \n",
        "                               \"scores\": scores }\n",
        "                result.append(frame_dict)               \n",
        "\n",
        "        return result\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "      #efficient method for doing face extraction on a single video\n",
        "        input_dir = os.path.dirname(video_path)\n",
        "        filenames = [ os.path.basename(video_path) ]\n",
        "        return self.process_videos(input_dir, filenames, [0])\n",
        "\n",
        "    def _tile_frames(self, frames, target_size):\n",
        "      # Splits each frame into several smaller, partially overlapping tiles\n",
        "      #   and resizes each tile to target_size\n",
        "       \n",
        "        num_frames, H, W, _ = frames.shape\n",
        "        split_size = min(H, W)\n",
        "        x_step = (W - split_size) // 2\n",
        "        y_step = (H - split_size) // 2\n",
        "        num_v = 1\n",
        "        num_h = 3 if W > H else 1\n",
        "\n",
        "        splits = np.zeros((num_frames * num_v * num_h, target_size[1], target_size[0], 3), dtype=np.uint8)\n",
        "\n",
        "        i = 0\n",
        "        for f in range(num_frames):\n",
        "            y = 0\n",
        "            for v in range(num_v):\n",
        "                x = 0\n",
        "                for h in range(num_h):\n",
        "                    crop = frames[f, y:y+split_size, x:x+split_size, :]\n",
        "                    splits[i] = cv2.resize(crop, target_size, interpolation=cv2.INTER_AREA)\n",
        "                    x += x_step\n",
        "                    i += 1\n",
        "                y += y_step\n",
        "\n",
        "        resize_info = [split_size / target_size[0], split_size / target_size[1], 0, 0]\n",
        "        return splits, resize_info\n",
        "\n",
        "    def _resize_detections(self, detections, target_size, resize_info):       \n",
        "        projected = []\n",
        "        target_w, target_h = target_size\n",
        "        scale_w, scale_h, offset_x, offset_y = resize_info\n",
        "\n",
        "        for i in range(len(detections)):\n",
        "            detection = detections[i].clone()          \n",
        "            for k in range(2):\n",
        "                detection[:, k*2    ] = (detection[:, k*2    ] * target_h - offset_y) * scale_h\n",
        "                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_w - offset_x) * scale_w\n",
        "\n",
        "            # keypoints are x,y\n",
        "            for k in range(2, 8):\n",
        "                detection[:, k*2    ] = (detection[:, k*2    ] * target_w - offset_x) * scale_w\n",
        "                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_h - offset_y) * scale_h\n",
        "\n",
        "            projected.append(detection)\n",
        "\n",
        "        return projected    \n",
        "    \n",
        "    def _untile_detections(self, num_frames, frame_size, detections):\n",
        "      # With N tiles per frame, there also are N times as many detections.\n",
        "      #   This function groups together the detections for a given frame; it is\n",
        "      #   the complement to tile_frames().\n",
        "       \n",
        "        combined_detections = []\n",
        "        W, H = frame_size\n",
        "        split_size = min(H, W)\n",
        "        x_step = (W - split_size) // 2\n",
        "        y_step = (H - split_size) // 2\n",
        "        num_v = 1\n",
        "        num_h = 3 if W > H else 1\n",
        "\n",
        "        i = 0\n",
        "        for f in range(num_frames):\n",
        "            detections_for_frame = []\n",
        "            y = 0\n",
        "            for v in range(num_v):\n",
        "                x = 0\n",
        "                for h in range(num_h):\n",
        "                    detection = detections[i].clone()\n",
        "                    if detection.shape[0] > 0:\n",
        "                        for k in range(2):\n",
        "                            detection[:, k*2    ] += y\n",
        "                            detection[:, k*2 + 1] += x\n",
        "                        for k in range(2, 8):\n",
        "                            detection[:, k*2    ] += x\n",
        "                            detection[:, k*2 + 1] += y\n",
        "\n",
        "                    detections_for_frame.append(detection)\n",
        "                    x += x_step\n",
        "                    i += 1\n",
        "                y += y_step\n",
        "\n",
        "            combined_detections.append(torch.cat(detections_for_frame))\n",
        "\n",
        "        return combined_detections\n",
        "    \n",
        "    def _add_margin_to_detections(self, detections, frame_size, margin=0.2):\n",
        "      #Expands the face bounding box\n",
        "      #The face detections often do not include the forehead, which\n",
        "      # is why we use twice the margin for ymin.\n",
        "      # Returns a PyTorch tensor of shape (num_detections, 17)\n",
        "        \n",
        "        offset = torch.round(margin * (detections[:, 2] - detections[:, 0]))\n",
        "        detections = detections.clone()\n",
        "        detections[:, 0] = torch.clamp(detections[:, 0] - offset*2, min=0)            # ymin\n",
        "        detections[:, 1] = torch.clamp(detections[:, 1] - offset, min=0)              # xmin\n",
        "        detections[:, 2] = torch.clamp(detections[:, 2] + offset, max=frame_size[1])  # ymax\n",
        "        detections[:, 3] = torch.clamp(detections[:, 3] + offset, max=frame_size[0])  # xmax\n",
        "        return detections\n",
        "    \n",
        "    def _crop_faces(self, frame, detections):\n",
        "      # Copies the face region(s) from the given frame into a set\n",
        "        # of new NumPy arrays\n",
        "        \n",
        "        faces = []\n",
        "        for i in range(len(detections)):\n",
        "            ymin, xmin, ymax, xmax = detections[i, :4].cpu().numpy().astype(np.int)\n",
        "            face = frame[ymin:ymax, xmin:xmax, :]\n",
        "            faces.append(face)\n",
        "        return faces\n",
        "\n",
        "    def remove_large_crops(self, crops, pct=0.1):\n",
        "      # Removes faces from the results if they take up more than X% \n",
        "        # of the video. Such a face is likely a false positive.\n",
        "        \n",
        "        for i in range(len(crops)):\n",
        "            frame_data = crops[i]\n",
        "            video_area = frame_data[\"frame_w\"] * frame_data[\"frame_h\"]\n",
        "            faces = frame_data[\"faces\"]\n",
        "            scores = frame_data[\"scores\"]\n",
        "            new_faces = []\n",
        "            new_scores = []\n",
        "            for j in range(len(faces)):\n",
        "                face = faces[j]\n",
        "                face_H, face_W, _ = face.shape\n",
        "                face_area = face_H * face_W\n",
        "                if face_area / video_area < 0.1:\n",
        "                    new_faces.append(face)\n",
        "                    new_scores.append(scores[j])\n",
        "            frame_data[\"faces\"] = new_faces\n",
        "            frame_data[\"scores\"] = new_scores\n",
        "\n",
        "    def keep_only_best_face(self, crops):\n",
        "        \n",
        "        for i in range(len(crops)):\n",
        "            frame_data = crops[i]\n",
        "            if len(frame_data[\"faces\"]) > 0:\n",
        "                frame_data[\"faces\"] = frame_data[\"faces\"][:1]\n",
        "                frame_data[\"scores\"] = frame_data[\"scores\"][:1]\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGLPYQaoDFn9"
      },
      "source": [
        "# **Detection**\n",
        "\n",
        "# !pip install pytorchcv\n",
        "import os, sys, time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPX1-nCxaMxF",
        "outputId": "42fbee18-e7fc-4f4b-cffc-e48a6476a1e0"
      },
      "source": [
        "!pip install /content/drive/MyDrive/deepfake/xception/pytorchcv-0.0.55-py2.py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./drive/MyDrive/deepfake/xception/pytorchcv-0.0.55-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorchcv==0.0.55) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorchcv==0.0.55) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv==0.0.55) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv==0.0.55) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv==0.0.55) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv==0.0.55) (1.24.3)\n",
            "Installing collected packages: pytorchcv\n",
            "Successfully installed pytorchcv-0.0.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPNV_pO4FlPq",
        "outputId": "3008337b-d40c-4e24-97c1-00fcd5b1f3a0"
      },
      "source": [
        "test_dir = \"/content/drive/MyDrive/deepfake/dataset/test_videos\"\n",
        "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
        "frame_h = 5\n",
        "frame_l = 5\n",
        "len(test_videos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm5eMpuIF4Wz",
        "outputId": "74a9dd97-72e8-46a8-a010-ce961f0b8a2d"
      },
      "source": [
        "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTtPt9HUGK6q"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/deepfake/blazeface\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htINxqZYGW_o"
      },
      "source": [
        "from blazeface import BlazeFace\n",
        "facedet = BlazeFace().to(gpu)\n",
        "facedet.load_weights(\"/content/drive/MyDrive/deepfake/blazeface/blazeface.pth\")\n",
        "facedet.load_anchors(\"/content/drive/MyDrive/deepfake/blazeface/anchors.npy\")\n",
        "_ = facedet.train(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5_yRM3XG90b"
      },
      "source": [
        "frames_per_video = 64 # originally 4\n",
        "\n",
        "video_reader = VideoReader()\n",
        "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "face_extractor = FaceExtractor(video_read_fn, facedet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilGUD11nGxza"
      },
      "source": [
        "input_size = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27r2Q5RgHIgs"
      },
      "source": [
        "from torchvision.transforms import Normalize\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdoc-QJiHMVU"
      },
      "source": [
        "def resize_image(img, size, resample=cv2.INTER_AREA):\n",
        "    h, w = img.shape[:2]\n",
        "    if w > h:\n",
        "        h = h * size // w\n",
        "        w = size\n",
        "    else:\n",
        "        w = w * size // h\n",
        "        h = size\n",
        "\n",
        "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def make_square_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    size = max(h, w)\n",
        "    t = 0\n",
        "    b = size - h\n",
        "    l = 0\n",
        "    r = size - w\n",
        "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhxmgm52HSCB"
      },
      "source": [
        "from pytorchcv.model_provider import get_model\n",
        "model = get_model(\"xception\", pretrained=False)\n",
        "model = nn.Sequential(*list(model.children())[:-1]) # Remove original output layer\n",
        "\n",
        "class Pooling(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Pooling, self).__init__()\n",
        "    \n",
        "    self.p1 = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.p2 = nn.AdaptiveMaxPool2d((1,1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.p1(x)\n",
        "    x2 = self.p2(x)\n",
        "    return (x1+x2) * 0.5\n",
        "\n",
        "model[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)))\n",
        "class Head(torch.nn.Module):\n",
        "  def __init__(self, in_f, out_f):\n",
        "    super(Head, self).__init__()\n",
        "    \n",
        "    self.f = nn.Flatten()\n",
        "    self.l = nn.Linear(in_f, 512)\n",
        "    self.d = nn.Dropout(0.5)\n",
        "    self.o = nn.Linear(512, out_f)\n",
        "    self.b1 = nn.BatchNorm1d(in_f)\n",
        "    self.b2 = nn.BatchNorm1d(512)\n",
        "    self.r = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.f(x)\n",
        "    x = self.b1(x)\n",
        "    x = self.d(x)\n",
        "\n",
        "    x = self.l(x)\n",
        "    x = self.r(x)\n",
        "    x = self.b2(x)\n",
        "    x = self.d(x)\n",
        "\n",
        "    out = self.o(x)\n",
        "    return out\n",
        "\n",
        "class FCN(torch.nn.Module):\n",
        "  def __init__(self, base, in_f):\n",
        "    super(FCN, self).__init__()\n",
        "    self.base = base\n",
        "    self.h1 = Head(in_f, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.base(x)\n",
        "    return self.h1(x)\n",
        "\n",
        "net = []\n",
        "model = FCN(model, 2048)\n",
        "model = model.cuda()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/deepfake/xception/model_50epochs_lr0001_patience5_factor01_batchsize32.pth')) # new, updated\n",
        "net.append(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGtyqX8nJJvf"
      },
      "source": [
        "def predict_on_video(video_path, batch_size):\n",
        "    try:\n",
        "        # Find the faces for N frames in the video.\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "\n",
        "        # Only look at one face per frame.\n",
        "        face_extractor.keep_only_best_face(faces)\n",
        "        \n",
        "        if len(faces) > 0:\n",
        "            # NOTE: When running on the CPU, the batch size must be fixed\n",
        "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
        "\n",
        "            # If we found any faces, prepare them for the model.\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    # Resize to the model's required input size.\n",
        "                    # We keep the aspect ratio intact and add zero\n",
        "                    # padding if necessary.                    \n",
        "                    resized_face = resize_image(face, input_size)\n",
        "                    resized_face = make_square_image(resized_face)\n",
        "\n",
        "                    if n < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
        "                    \n",
        "                    # Test time augmentation: horizontal flips.\n",
        "                    # TODO: not sure yet if this helps or not\n",
        "                    #x[n] = cv2.flip(resized_face, 1)\n",
        "                    #n += 1\n",
        "\n",
        "            if n > 0:\n",
        "                x = torch.tensor(x, device=gpu).float()\n",
        "\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.)\n",
        "#                     x[i] = x[i] / 255.\n",
        "\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    y_pred = model(x)\n",
        "                    y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                    return y_pred[:n].mean().item()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPoDQ_p0JR6W"
      },
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def predict_on_video_set(videos, num_workers):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos)))\n",
        "\n",
        "    return list(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEVTthXoNyT5"
      },
      "source": [
        "model.eval()\n",
        "predictions = predict_on_video_set(test_videos, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpOCJbA6J2Ag"
      },
      "source": [
        "prediction_value = []\n",
        "for value in predictions:\n",
        "    if value > .60:\n",
        "        prediction_value.append('FAKE')\n",
        "    else:\n",
        "        prediction_value.append('REAL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFoqdE6sJd4i"
      },
      "source": [
        "submission_df_xception = pd.DataFrame({\"filename\": test_videos, \"label\": predictions,\"result\":prediction_value})\n",
        "submission_df_xception.to_csv(\"/content/drive/MyDrive/deepfake/output/result.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5k5KNrbVJrNH",
        "outputId": "ed26fd19-89a7-41fc-ec94-f7f49f4e9ab5"
      },
      "source": [
        "submission_df_xception.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>label</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aassnaulhq.mp4</td>\n",
              "      <td>0.971394</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aayfryxljh.mp4</td>\n",
              "      <td>0.009612</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>acazlolrpz.mp4</td>\n",
              "      <td>0.871641</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adohdulfwb.mp4</td>\n",
              "      <td>0.005082</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ahjnxtiamx.mp4</td>\n",
              "      <td>0.748403</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         filename     label result\n",
              "0  aassnaulhq.mp4  0.971394   FAKE\n",
              "1  aayfryxljh.mp4  0.009612   REAL\n",
              "2  acazlolrpz.mp4  0.871641   FAKE\n",
              "3  adohdulfwb.mp4  0.005082   REAL\n",
              "4  ahjnxtiamx.mp4  0.748403   FAKE"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}